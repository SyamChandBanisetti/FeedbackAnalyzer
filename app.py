import streamlit as st
import pandas as pd
import plotly.express as px
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import os
import io
from datetime import datetime
import base64
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# --- Streamlit Page Configuration ---
st.set_page_config(
    page_title="Feedback Analyzer Tool",
    page_icon="📋",
    layout="wide"
)

# --- Constants and Configuration ---
STOP_WORDS = set([
    "a", "an", "the", "and", "but", "or", "to", "of", "in", "on", "at", "for", "with",
    "as", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had",
    "do", "does", "did", "not", "no", "don't", "can't", "won't", "shouldn't", "wouldn't",
    "couldn't", "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us",
    "them", "my", "your", "his", "its", "our", "their", "this", "that", "these", "those",
    "some", "any", "all", "each", "every", "many", "much", "more", "most", "other",
    "such", "only", "own", "so", "than", "too", "very", "s", "t", "can", "will", "just",
    "don", "should", "now", "ve", "ll", "m", "re", "d", "again", "further", "then",
    "once", "here", "there", "when", "where", "why", "how", "all", "any", "both",
    "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only",
    "own", "same", "so", "than", "too", "very", "great", "good", "bad", "poor", "issue", "problem", "really", "very", "much",
    "like", "get", "got", "would", "could", "said", "say", "also", "well", "from", "into",
    "through", "during", "before", "after", "above", "below", "up", "down", "out", "off",
    "over", "under", "again", "further", "then", "once", "here", "there", "when",
    "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other",
    "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very",
    "just", "don", "should", "now", "d", "ll", "m", "o", "re", "ve", "y", "ain", "aren",
    "couldn", "didn", "doesn", "hadn", "hasn", "haven", "isn", "ma", "mightn", "mustn",
    "needn", "shan", "shouldn", "wasn", "weren", "won", "wouldn", "etc"
])

EMOTION_KEYWORDS = {
    "Joy": ["happy", "delighted", "pleased", "excited", "joyful", "satisfied", "great", "love"],
    "Sadness": ["sad", "unhappy", "down", "depressed", "disappointed", "frustrated", "grief"],
    "Anger": ["angry", "mad", "frustrated", "annoyed", "irritated", "furious", "rage"],
    "Surprise": ["surprised", "shocked", "amazed", "unexpected", "astonished"],
    "Fear": ["fear", "scared", "anxious", "worried", "nervous", "dread"],
    "Disgust": ["disgusted", "revolted", "sick", "nauseated", "repulsed"],
    "Trust": ["trust", "reliable", "confident", "secure", "faithful"],
    "Anticipation": ["eager", "expecting", "hopeful", "waiting", "looking forward"]
}

CES_KEYWORDS = {
    "Low Effort": ["easy", "simple", "smooth", "seamless", "effortless", "quick", "no hassle", "straightforward"],
    "High Effort": ["difficult", "hard", "complicated", "frustrating", "time-consuming", "struggle", "challenging", "complex"]
}

# --- Gemini API Configuration ---
gemini_api_key = None
try:
    # Attempt to load from Streamlit secrets (for Streamlit Cloud)
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
except KeyError:
    # If not in secrets, try from environment variables (for local deployment)
    gemini_api_key = os.getenv("GEMINI_API_KEY")

if not gemini_api_key:
    st.warning("🔐 Gemini API Key not found. Please ensure it's set in Streamlit Cloud secrets or as an environment variable (e.g., in a `.env` file for local testing).")
    st.session_state['gemini_model'] = None # Ensure model is not initialized if key is missing
else:
    # Initialize Gemini model if API key is available
    try:
        genai.configure(api_key=gemini_api_key)
        st.session_state['gemini_model'] = genai.GenerativeModel(
            model_name="gemini-2.0-flash",
            safety_settings={
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            }
        )
        # Simple test prompt to check connectivity and initial blocking
        try:
            test_response = st.session_state['gemini_model'].generate_content("Hello, how are you today?")
            if test_response.candidates:
                st.sidebar.success("✅ Connected to Gemini API.")
            else:
                st.sidebar.error("Gemini API connected but no candidates returned for test prompt.")
                if test_response.prompt_feedback and test_response.prompt_feedback.block_reason:
                    st.sidebar.error(f"Test prompt blocked for reason: {test_response.prompt_feedback.block_reason.name}")
        except Exception as e:
            st.sidebar.error(f"Error during Gemini test prompt: {e}")
            st.session_state['gemini_model'] = None # Invalidate model if test fails
    except Exception as e:
        st.sidebar.error(f"Error initializing Gemini: '{e}'. Check your API key or network.")
        st.session_state['gemini_model'] = None # Invalidate model if initialization fails
        

# --- Helper Functions ---

@st.cache_data
def load_data(uploaded_file):
    """Loads data from CSV or Excel."""
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        elif uploaded_file.name.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(uploaded_file)
        else:
            st.error("Unsupported file type. Please upload a CSV or Excel file.")
            return None
        return df
    except Exception as e:
        st.error(f"Error loading file: {e}")
        return None

@st.cache_data
def analyze_sentiment(text_series):
    """Simple keyword-based sentiment analysis."""
    positive_keywords = ["good", "great", "excellent", "positive", "happy", "love", "satisfied", "recommend", "best", "smooth", "easy"]
    negative_keywords = ["bad", "poor", "terrible", "negative", "unhappy", "hate", "dissatisfied", "frustrating", "problem", "issue", "difficult"]

    sentiments = []
    for text in text_series.astype(str).fillna(""):
        text_lower = text.lower()
        if any(keyword in text_lower for keyword in positive_keywords) and not any(keyword in text_lower for keyword in negative_keywords):
            sentiments.append("Positive")
        elif any(keyword in text_lower for keyword in negative_keywords) and not any(keyword in text_lower for keyword in positive_keywords):
            sentiments.append("Negative")
        elif any(keyword in text_lower for keyword in positive_keywords) and any(keyword in text_lower for keyword in negative_keywords):
            sentiments.append("Mixed") # Both positive and negative keywords
        else:
            sentiments.append("Neutral")
    return sentiments

@st.cache_data
def get_top_n_grams(text_series, n=1, top_n=10):
    """Generates top N-grams (single words, bigrams, trigrams)."""
    vectorizer = TfidfVectorizer(
        ngram_range=(n, n),
        stop_words=list(STOP_WORDS),
        token_pattern=r'\b[a-zA-Z]{2,}\b' # Only words, min 2 chars
    )
    try:
        tfidf_matrix = vectorizer.fit_transform(text_series.astype(str).fillna(""))
        feature_names = vectorizer.get_feature_names_out()
        sums = tfidf_matrix.sum(axis=0)
        data = []
        for col, term in enumerate(feature_names):
            data.append((term, sums[0, col]))
        ranking = pd.DataFrame(data, columns=['term', 'tfidf']).sort_values(by='tfidf', ascending=False)
        return ranking.head(top_n)
    except ValueError:
        return pd.DataFrame(columns=['term', 'tfidf'])

@st.cache_data
def analyze_response_length(text_series):
    """Calculates word count for each response."""
    return text_series.astype(str).fillna("").apply(lambda x: len(x.split()))

@st.cache_data
def detect_emotions(text_series):
    """Detects emotions based on keywords."""
    emotion_counts = {emotion: 0 for emotion in EMOTION_KEYWORDS}
    total_responses = len(text_series)
    if total_responses == 0:
        return {emotion: 0 for emotion in EMOTION_KEYWORDS} # Return zeros if no responses

    for text in text_series.astype(str).fillna(""):
        text_lower = text.lower()
        found_emotion_in_response = False
        for emotion, keywords in EMOTION_KEYWORDS.items():
            if any(keyword in text_lower for keyword in keywords):
                emotion_counts[emotion] += 1
                found_emotion_in_response = True
        # If no specific emotion keyword found, consider it neutral for emotion analysis
        if not found_emotion_in_response:
            # We don't increment a "neutral" counter here, as it's about detecting *specific* emotions.
            pass
    
    # Convert counts to percentages
    emotion_percentages = {
        emotion: (count / total_responses) * 100 if total_responses > 0 else 0
        for emotion, count in emotion_counts.items()
    }
    return emotion_percentages


@st.cache_data
def calculate_ces(text_series):
    """Approximates Customer Effort Score based on keywords."""
    effort_scores = []
    for text in text_series.astype(str).fillna(""):
        text_lower = text.lower()
        low_effort_matches = sum(1 for keyword in CES_KEYWORDS["Low Effort"] if keyword in text_lower)
        high_effort_matches = sum(1 for keyword in CES_KEYWORDS["High Effort"] if keyword in text_lower)

        if low_effort_matches > high_effort_matches:
            effort_scores.append("Low Effort")
        elif high_effort_matches > low_effort_matches:
            effort_scores.append("High Effort")
        else:
            effort_scores.append("Neutral/Mixed Effort")
    return effort_scores

@st.cache_data
def generate_wordcloud(text_data):
    """Generates a word cloud image."""
    if not text_data:
        return None
    # Ensure text is string and handle potential non-string elements
    text_data_str = " ".join([str(item) for item in text_data if pd.notna(item)])
    text_data_str = text_data_str.strip() # Remove leading/trailing whitespace

    # Explicit check for empty string after cleaning
    if not text_data_str:
        return None # No words to generate word cloud from

    wordcloud = WordCloud(width=800, height=400, background_color='white',
                          stopwords=STOP_WORDS, min_font_size=10).generate(text_data_str)
    
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis("off")
    return fig

@st.cache_data
def generate_co_occurrence_graph(text_series, min_cooccurrence=2):
    """Generates a word co-occurrence graph."""
    sentences = text_series.astype(str).fillna("").apply(lambda x: x.lower().split())
    filtered_sentences = [[word for word in sentence if word not in STOP_WORDS and len(word) > 1] for sentence in sentences]

    co_occurrence_matrix = {}
    for sentence in filtered_sentences:
        for i, word1 in enumerate(sentence):
            if word1 not in co_occurrence_matrix:
                co_occurrence_matrix[word1] = {}
            for j, word2 in enumerate(sentence):
                if i != j:
                    if word2 not in co_occurrence_matrix[word1]:
                        co_occurrence_matrix[word1][word2] = 0
                    co_occurrence_matrix[word1][word2] += 1

    G = nx.Graph()
    for word1, connections in co_occurrence_matrix.items():
        for word2, count in connections.items():
            if count >= min_cooccurrence:
                G.add_edge(word1, word2, weight=count)

    if not G.edges():
        return None, "No strong co-occurrences found with current settings."

    pos = nx.spring_layout(G, k=0.15, iterations=20, seed=42)
    fig, ax = plt.subplots(figsize=(12, 10))
    
    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
    node_sizes = [G.degree(node) * 100 for node in G.nodes()] # Size nodes by their degree
    
    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', alpha=0.9, ax=ax)
    nx.draw_networkx_edges(G, pos, width=[w * 0.5 for w in edge_weights], alpha=0.5, edge_color='gray', ax=ax)
    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold', ax=ax)
    
    ax.set_title(f'Word Co-occurrence Network (Min Co-occurrence: {min_cooccurrence})')
    ax.axis("off")
    return fig, None

# --- Gemini AI Summarization Function ---
@st.cache_data(show_spinner="Generating AI Summary with Gemini...")
def get_gemini_summary(text_to_summarize, model):
    if not model:
        return "Gemini AI is not available. Please provide a valid API key."

    try:
        # Gemini often prefers list of strings for content, even for single requests
        response = model.generate_content([f"Summarize the following feedback comments concisely, highlight key themes, and identify actionable insights:\n\n{text_to_summarize}"])
        if response.candidates:
            # st.write("Gemini Raw Response:", response) # Debugging line
            # st.write("Gemini Prompt Feedback:", response.prompt_feedback) # Debugging line
            return response.text
        else:
            block_reason = "Unknown reason"
            if response.prompt_feedback and response.prompt_feedback.block_reason:
                block_reason = response.prompt_feedback.block_reason.name
            return f"Gemini did not return a summary. Content may have been blocked due to safety concerns. Block reason: '{block_reason}'."
    except Exception as e:
        return f"Error connecting to Gemini for summary: '{e}'. This might be due to safety filters or an API issue."

# --- HTML Report Generation ---
def create_html_report(df, selected_text_columns, analyses_results, analysis_selections):
    html_content = f"""
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Feedback Analysis Report - {datetime.now().strftime('%Y-%m-%d %H:%M')}</title>
        <style>
            body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; margin: 20px; }}
            h1, h2, h3 {{ color: #005f73; border-bottom: 2px solid #e0e0e0; padding-bottom: 5px; margin-top: 30px; }}
            .container {{ max-width: 1200px; margin: auto; padding: 20px; background: #fff; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }}
            .section {{ margin-bottom: 40px; }}
            .chart-img {{ max-width: 100%; height: auto; display: block; margin: 15px auto; border: 1px solid #ddd; }}
            table {{ width: 100%; border-collapse: collapse; margin-top: 15px; }}
            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
            th {{ background-color: #f2f2f2; }}
            .st-dataframe {{ overflow-x: auto; }} /* Allow horizontal scroll for dataframes */
            .info-box {{ background-color: #e6f7ff; border-left: 5px solid #2196f3; padding: 10px; margin-bottom: 15px; }}
            .warning-box {{ background-color: #fff3e0; border-left: 5px solid #ff9800; padding: 10px; margin-bottom: 15px; }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>Feedback Analysis Report</h1>
            <p><strong>Report Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p>This report provides an in-depth analysis of customer feedback data.</p>

            <div class="section">
                <h2>1. Data Overview</h2>
                <h3>Original Data Sample ({df.shape[0]} rows)</h3>
                <div class="st-dataframe">
                    {df.head().to_html()}
                </div>
                <p>Selected text columns for analysis: {', '.join(selected_text_columns)}</p>
            </div>

            {" " if analysis_selections.get('sentiment_analysis') else ""}
            <div class="section">
                <h2>2. Sentiment Analysis</h2>
                {analyses_results.get('sentiment_analysis', '<p>Sentiment analysis was not selected or no data.</p>')}
            </div>

            {" " if analysis_selections.get('key_phrase_analysis') else ""}
            <div class="section">
                <h2>3. Key Phrase Analysis</h2>
                <h3>Top Keywords (Unigrams)</h3>
                {analyses_results.get('top_keywords_html', '<p>Key phrase analysis was not selected or no data.</p>')}
                <h3>Top Phrases (Bigrams)</h3>
                {analyses_results.get('top_bigrams_html', '')}
                <h3>Top Phrases (Trigrams)</h3>
                {analyses_results.get('top_trigrams_html', '')}
            </div>

            {" " if analysis_selections.get('response_length_analysis') else ""}
            <div class="section">
                <h2>4. Response Length Analysis</h2>
                {analyses_results.get('response_length_summary', '<p>Response length analysis was not selected or no data.</p>')}
            </div>

            {" " if analysis_selections.get('emotion_detection') else ""}
            <div class="section">
                <h2>5. Emotion Detection</h2>
                {analyses_results.get('emotion_analysis_summary', '<p>Emotion detection was not selected or no data.</p>')}
            </div>

            {" " if analysis_selections.get('ces_approximation') else ""}
            <div class="section">
                <h2>6. Customer Effort Score (CES) Approximation</h2>
                {analyses_results.get('ces_analysis_summary', '<p>CES approximation was not selected or no data.</p>')}
            </div>
            
            {" " if analysis_selections.get('word_cloud') else ""}
            <div class="section">
                <h2>7. Word Cloud</h2>
                {analyses_results.get('wordcloud_image', '<p>Word cloud generation was not selected or no data.</p>')}
            </div>

            {" " if analysis_selections.get('co_occurrence_network') else ""}
            <div class="section">
                <h2>8. Word Co-occurrence Network</h2>
                {analyses_results.get('co_occurrence_graph', '<p>Co-occurrence network generation was not selected or no data.</p>')}
            </div>

            {" " if analysis_selections.get('gemini_summarization') else ""}
            <div class="section">
                <h2>9. Gemini AI Summaries</h2>
                {analyses_results.get('gemini_summary_overall', '<p>Gemini AI summarization was not selected or no summary generated.</p>')}
                {analyses_results.get('gemini_summary_questions', '')}
            </div>
        </div>
    </body>
    </html>
    """
    return html_content

def fig_to_base64(fig):
    """Converts a matplotlib figure to a base64 encoded image."""
    buf = io.BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight', dpi=300)
    buf.seek(0)
    img_base64 = base64.b64encode(buf.read()).decode('utf-8')
    plt.close(fig) # Close the figure to free up memory
    return f'<img src="data:image/png;base64,{img_base64}" class="chart-img"/>'


# --- Main Streamlit Application ---

st.title("📋 Feedback Analyzer Tool")
st.markdown("Upload your feedback data (CSV or Excel) to get insights into sentiments, key phrases, emotions, and more, powered by Gemini AI.")

uploaded_file = st.sidebar.file_uploader("Upload your CSV or Excel file", type=["csv", "xls", "xlsx"], key="file_uploader")

# Initialize session state for checkboxes if not already present
if 'analyze_sentiment_analysis' not in st.session_state:
    st.session_state.analyze_sentiment_analysis = True
if 'analyze_key_phrase_analysis' not in st.session_state:
    st.session_state.analyze_key_phrase_analysis = True
if 'analyze_response_length_analysis' not in st.session_state:
    st.session_state.analyze_response_length_analysis = True
if 'analyze_emotion_detection' not in st.session_state:
    st.session_state.analyze_emotion_detection = True
if 'analyze_ces_approximation' not in st.session_state:
    st.session_state.analyze_ces_approximation = True
if 'analyze_word_cloud' not in st.session_state:
    st.session_state.analyze_word_cloud = True
if 'analyze_co_occurrence_network' not in st.session_state:
    st.session_state.analyze_co_occurrence_network = True
if 'analyze_gemini_summarization' not in st.session_state:
    st.session_state.analyze_gemini_summarization = True


if uploaded_file:
    df = load_data(uploaded_file)
    if df is not None:
        st.sidebar.success("File uploaded successfully!")
        st.sidebar.dataframe(df.head(), use_container_width=True, key="sidebar_df_head")

        text_columns = [col for col in df.columns if df[col].dtype == 'object']
        if not text_columns:
            st.error("No text (object) columns found in your data. Please ensure your feedback is in a text format.")
            st.stop()

        selected_text_columns = st.sidebar.multiselect(
            "Select text columns for analysis:",
            options=text_columns,
            default=text_columns[0] if text_columns else [],
            key="column_multiselect"
        )

        if not selected_text_columns:
            st.warning("Please select at least one text column for analysis.")
            st.stop()

        # Combine selected text columns into a single series for overall analysis
        combined_feedback_series = df[selected_text_columns].astype(str).fillna('').agg(' '.join, axis=1)
        full_text_for_analysis = " ".join(combined_feedback_series.dropna().tolist())

        st.header("Overall Feedback Analysis")
        st.info("Results below are based on the combined text from selected columns. Use checkboxes in the sidebar to select analyses.")

        # --- Analysis Checkboxes in Sidebar ---
        st.sidebar.subheader("Select Analyses to Perform")
        st.session_state.analyze_sentiment_analysis = st.sidebar.checkbox("Sentiment Analysis", value=st.session_state.analyze_sentiment_analysis, key="chk_sentiment")
        st.session_state.analyze_key_phrase_analysis = st.sidebar.checkbox("Key Phrase Analysis (N-grams)", value=st.session_state.analyze_key_phrase_analysis, key="chk_key_phrases")
        st.session_state.analyze_response_length_analysis = st.sidebar.checkbox("Response Length Analysis", value=st.session_state.analyze_response_length_analysis, key="chk_response_length")
        st.session_state.analyze_emotion_detection = st.sidebar.checkbox("Emotion Detection", value=st.session_state.analyze_emotion_detection, key="chk_emotion")
        st.session_state.analyze_ces_approximation = st.sidebar.checkbox("Customer Effort Score (CES)", value=st.session_state.analyze_ces_approximation, key="chk_ces")
        st.session_state.analyze_word_cloud = st.sidebar.checkbox("Word Cloud", value=st.session_state.analyze_word_cloud, key="chk_word_cloud")
        st.session_state.analyze_co_occurrence_network = st.sidebar.checkbox("Word Co-occurrence Network", value=st.session_state.analyze_co_occurrence_network, key="chk_co_occurrence")
        st.session_state.analyze_gemini_summarization = st.sidebar.checkbox("Gemini AI Summarization", value=st.session_state.analyze_gemini_summarization, key="chk_gemini_summary")

        # --- Display Raw Data (Optional) ---
        if st.checkbox("Show Raw Data Sample", key="show_raw_data_checkbox"):
            st.subheader("Raw Data Sample")
            st.dataframe(df.head(10), use_container_width=True, key="raw_data_df")

        # --- Sentiment Analysis ---
        if st.session_state.analyze_sentiment_analysis:
            st.subheader("Sentiment Analysis")
            with st.spinner("Analyzing sentiments..."):
                sentiment_df = pd.DataFrame()
                for col in selected_text_columns:
                    sentiment_df[f'{col}_Sentiment'] = analyze_sentiment(df[col])
                
                # Combine all sentiment columns into a single Series for overall count
                overall_sentiments = pd.concat([sentiment_df[col] for col in sentiment_df.columns])
                sentiment_counts = overall_sentiments.value_counts(normalize=True) * 100
                
                if not sentiment_counts.empty:
                    fig_sentiment = px.pie(
                        values=sentiment_counts.values,
                        names=sentiment_counts.index,
                        title="Overall Sentiment Distribution",
                        hole=0.4
                    )
                    fig_sentiment.update_traces(textposition='inside', textinfo='percent+label')
                    st.plotly_chart(fig_sentiment, use_container_width=True, key="overall_sentiment_pie_chart") 
                    
                    st.write("Sentiment Breakdown:")
                    st.dataframe(sentiment_counts.reset_index().rename(columns={'index': 'Sentiment', 0: 'Percentage'}), use_container_width=True, key="overall_sentiment_df")
                else:
                    st.info("Not enough data to perform sentiment analysis.")

        # --- Key Phrase Analysis ---
        if st.session_state.analyze_key_phrase_analysis:
            st.subheader("Key Phrase Analysis")
            col1, col2, col3 = st.columns(3)

            with col1:
                st.markdown("### Top Keywords (Unigrams)")
                top_keywords = get_top_n_grams(combined_feedback_series, n=1, top_n=15)
                st.dataframe(top_keywords, use_container_width=True, key="top_keywords_df")

            with col2:
                st.markdown("### Top Phrases (Bigrams)")
                top_bigrams = get_top_n_grams(combined_feedback_series, n=2, top_n=10)
                st.dataframe(top_bigrams, use_container_width=True, key="top_bigrams_df")

            with col3:
                st.markdown("### Top Phrases (Trigrams)")
                top_trigrams = get_top_n_grams(combined_feedback_series, n=3, top_n=5)
                st.dataframe(top_trigrams, use_container_width=True, key="top_trigrams_df")

        # --- Response Length Analysis (Direct Representation) ---
        if st.session_state.analyze_response_length_analysis:
            st.subheader("Response Length Analysis")
            response_lengths = analyze_response_length(combined_feedback_series)
            if not response_lengths.empty:
                st.write(f"**Average response length:** {response_lengths.mean():.2f} words")
                st.write(f"**Median response length:** {response_lengths.median():.0f} words")
                st.write(f"**Minimum response length:** {response_lengths.min():.0f} words")
                st.write(f"**Maximum response length:** {response_lengths.max():.0f} words")
                st.markdown("---")
                st.markdown("##### Response Length Summary Statistics")
                st.dataframe(response_lengths.describe().to_frame().T, use_container_width=True, key="response_length_summary_df")
            else:
                st.info("Not enough data to analyze response lengths.")


        # --- Emotion Detection (Direct Representation) ---
        if st.session_state.analyze_emotion_detection:
            st.subheader("Emotion Detection (Keyword-Based)")
            with st.spinner("Detecting emotions..."):
                emotion_percentages = detect_emotions(combined_feedback_series)
                emotion_df = pd.DataFrame(emotion_percentages.items(), columns=['Emotion', 'Percentage']).sort_values(by='Percentage', ascending=False)
                
                if not emotion_df.empty and emotion_df['Percentage'].sum() > 0:
                    st.write("Percentage of responses expressing each emotion:")
                    st.dataframe(emotion_df, use_container_width=True, key="emotion_df")
                    st.info("Note: A response can express multiple emotions if it contains keywords from different categories.")
                else:
                    st.info("No specific emotion keywords detected in the feedback.")

        # --- Customer Effort Score (CES) Approximation (Direct Representation) ---
        if st.session_state.analyze_ces_approximation:
            st.subheader("Customer Effort Score (CES) Approximation")
            with st.spinner("Approximating CES..."):
                ces_scores = calculate_ces(combined_feedback_series)
                ces_counts = pd.Series(ces_scores).value_counts(normalize=True) * 100
                
                if not ces_counts.empty and ces_counts.sum() > 0:
                    st.write("Distribution of approximated Customer Effort Levels:")
                    st.dataframe(ces_counts.reset_index().rename(columns={'index': 'Effort Level', 0: 'Percentage'}), use_container_width=True, key="ces_df")
                    st.info("Note: CES is approximated based on the presence of predefined keywords related to effort.")
                else:
                    st.info("Not enough data to approximate Customer Effort Score.")


        # --- Word Cloud ---
        if st.session_state.analyze_word_cloud:
            st.subheader("Word Cloud")
            with st.spinner("Generating word cloud..."):
                text_for_wordcloud = " ".join(combined_feedback_series.dropna().tolist())
                wordcloud_fig = generate_wordcloud(text_for_wordcloud)
                if wordcloud_fig:
                    st.pyplot(wordcloud_fig, key="wordcloud_plot")
                else:
                    st.info("Not enough relevant text to generate a word cloud.")

        # --- Word Co-occurrence Network ---
        if st.session_state.analyze_co_occurrence_network:
            st.subheader("Word Co-occurrence Network")
            min_cooccurrence = st.slider("Minimum co-occurrence for graph:", 1, 10, 2, key="min_cooccurrence_slider")
            with st.spinner("Generating co-occurrence graph..."):
                co_occurrence_fig, co_occurrence_message = generate_co_occurrence_graph(combined_feedback_series, min_cooccurrence)
                if co_occurrence_fig:
                    st.pyplot(co_occurrence_fig, key="cooccurrence_plot")
                else:
                    st.info(co_occurrence_message if co_occurrence_message else "No co-occurrences found to display a graph.")

        # --- Gemini AI Summarization ---
        if st.session_state.analyze_gemini_summarization:
            st.subheader("Gemini AI Summarization")
            if st.session_state.get('gemini_model'): # Check if model was successfully initialized
                if st.button("Generate Overall AI Summary", key="generate_overall_ai_summary_btn"):
                    with st.spinner("Asking Gemini to summarize overall feedback..."):
                        overall_summary = get_gemini_summary(full_text_for_analysis, st.session_state['gemini_model'])
                        st.markdown("#### Overall Summary by Gemini AI")
                        st.write(overall_summary)
                        # Store for report
                        st.session_state['overall_gemini_summary_generated'] = overall_summary 

                st.markdown("#### Summaries for Individual Feedback Questions (AI)")
                for col in selected_text_columns:
                    question_text = df[col].astype(str).fillna("").tolist()
                    question_text_filtered = [text for text in question_text if text.strip()]
                    if question_text_filtered:
                        question_feedback_combined = "\n".join(question_text_filtered)
                        with st.expander(f"Generate Summary for: {col}", key=f"expander_{col}"):
                            if st.button(f"Summarize '{col}'", key=f"summarize_btn_{col}"):
                                with st.spinner(f"Asking Gemini to summarize '{col}'..."):
                                    question_summary = get_gemini_summary(question_feedback_combined, st.session_state['gemini_model'])
                                    st.write(question_summary)
                                    st.session_state[f'gemini_summary_{col}'] = question_summary # Store for report
                    else:
                        st.info(f"No valid text responses in column '{col}' to summarize.")
            else:
                st.warning("Gemini AI features are disabled because the API key was not found or the connection failed. Please ensure your API key is correctly configured in Streamlit Cloud secrets or environment variables.")

        # --- Generate and Download HTML Report ---
        st.sidebar.markdown("---")
        st.sidebar.subheader("Download Report")

        if st.sidebar.button("Generate HTML Report", key="generate_html_report_btn"):
            with st.spinner("Generating comprehensive report..."):
                analyses_for_report = {}
                analysis_selections = {
                    'sentiment_analysis': st.session_state.analyze_sentiment_analysis,
                    'key_phrase_analysis': st.session_state.analyze_key_phrase_analysis,
                    'response_length_analysis': st.session_state.analyze_response_length_analysis,
                    'emotion_detection': st.session_state.analyze_emotion_detection,
                    'ces_approximation': st.session_state.analyze_ces_approximation,
                    'word_cloud': st.session_state.analyze_word_cloud,
                    'co_occurrence_network': st.session_state.analyze_co_occurrence_network,
                    'gemini_summarization': st.session_state.analyze_gemini_summarization,
                }

                # 1. Sentiment Analysis
                if analysis_selections['sentiment_analysis'] and 'sentiment_counts' in locals() and not sentiment_counts.empty:
                    sentiment_counts_html = sentiment_counts.reset_index().rename(columns={'index': 'Sentiment', 0: 'Percentage'}).to_html()
                    fig_sentiment_report = px.pie(values=sentiment_counts.values, names=sentiment_counts.index, title="Overall Sentiment Distribution", hole=0.4)
                    fig_sentiment_report.update_traces(textposition='inside', textinfo='percent+label')
                    analyses_for_report['sentiment_analysis'] = f"<h3>Overall Sentiment Distribution</h3>{fig_to_base64(fig_sentiment_report)}<p>Sentiment Breakdown:</p>{sentiment_counts_html}"
                else:
                    analyses_for_report['sentiment_analysis'] = '<p>Sentiment analysis was not selected or no data.</p>'

                # 2. Key Phrase Analysis
                if analysis_selections['key_phrase_analysis']:
                    # Ensure top_keywords, top_bigrams, top_trigrams are available or re-run them for report if needed
                    # (assuming they were already generated if checkbox was true)
                    top_keywords_report = get_top_n_grams(combined_feedback_series, n=1, top_n=15)
                    top_bigrams_report = get_top_n_grams(combined_feedback_series, n=2, top_n=10)
                    top_trigrams_report = get_top_n_grams(combined_feedback_series, n=3, top_n=5)

                    analyses_for_report['top_keywords_html'] = top_keywords_report.to_html() if not top_keywords_report.empty else '<p>No top keywords data.</p>'
                    analyses_for_report['top_bigrams_html'] = top_bigrams_report.to_html() if not top_bigrams_report.empty else '<p>No top bigrams data.</p>'
                    analyses_for_report['top_trigrams_html'] = top_trigrams_report.to_html() if not top_trigrams_report.empty else '<p>No top trigrams data.</p>'
                else:
                    analyses_for_report['top_keywords_html'] = '<p>Key phrase analysis was not selected.</p>'
                    analyses_for_report['top_bigrams_html'] = ''
                    analyses_for_report['top_trigrams_html'] = ''

                # 3. Response Length (Report version)
                if analysis_selections['response_length_analysis']:
                    response_lengths_report = analyze_response_length(combined_feedback_series)
                    if not response_lengths_report.empty:
                        length_stats_html = response_lengths_report.describe().to_frame().T.to_html()
                        analyses_for_report['response_length_summary'] = f"<h3>Response Length Analysis</h3>" \
                                                                           f"<p><strong>Average:</strong> {response_lengths_report.mean():.2f} words</p>" \
                                                                           f"<p><strong>Median:</strong> {response_lengths_report.median():.0f} words</p>" \
                                                                           f"<p><strong>Minimum:</strong> {response_lengths_report.min():.0f} words</p>" \
                                                                           f"<p><strong>Maximum:</strong> {response_lengths_report.max():.0f} words</p>" \
                                                                           f"<h4>Summary Statistics</h4>{length_stats_html}"
                    else:
                        analyses_for_report['response_length_summary'] = '<p>No response length data.</p>'
                else:
                    analyses_for_report['response_length_summary'] = '<p>Response length analysis was not selected.</p>'

                # 4. Emotion Detection (Report version)
                if analysis_selections['emotion_detection']:
                    emotion_percentages_report = detect_emotions(combined_feedback_series)
                    emotion_df_report = pd.DataFrame(emotion_percentages_report.items(), columns=['Emotion', 'Percentage']).sort_values(by='Percentage', ascending=False)
                    if not emotion_df_report.empty and emotion_df_report['Percentage'].sum() > 0:
                        analyses_for_report['emotion_analysis_summary'] = f"<h3>Emotion Detection (Keyword-Based)</h3>" \
                                                                           f"<h4>Percentage of responses expressing each emotion:</h4>" \
                                                                           f"{emotion_df_report.to_html()}" \
                                                                           f"<p><i>Note: A response can express multiple emotions if it contains keywords from different categories.</i></p>"
                    else:
                        analyses_for_report['emotion_analysis_summary'] = '<p>No emotion analysis data.</p>'
                else:
                    analyses_for_report['emotion_analysis_summary'] = '<p>Emotion detection was not selected.</p>'

                # 5. CES Approximation (Report version)
                if analysis_selections['ces_approximation']:
                    ces_scores_report = calculate_ces(combined_feedback_series)
                    ces_counts_report = pd.Series(ces_scores_report).value_counts(normalize=True) * 100
                    if not ces_counts_report.empty and ces_counts_report.sum() > 0:
                        analyses_for_report['ces_analysis_summary'] = f"<h3>Customer Effort Score (CES) Approximation</h3>" \
                                                                       f"<h4>Distribution of approximated Customer Effort Levels:</h4>" \
                                                                       f"{ces_counts_report.reset_index().rename(columns={'index': 'Effort Level', 0: 'Percentage'}).to_html()}" \
                                                                       f"<p><i>Note: CES is approximated based on the presence of predefined keywords related to effort.</i></p>"
                    else:
                        analyses_for_report['ces_analysis_summary'] = '<p>No CES analysis data.</p>'
                else:
                    analyses_for_report['ces_analysis_summary'] = '<p>CES approximation was not selected.</p>'

                # 6. Word Cloud
                if analysis_selections['word_cloud']:
                    text_for_wordcloud_report = " ".join(combined_feedback_series.dropna().tolist())
                    wordcloud_fig_report = generate_wordcloud(text_for_wordcloud_report)
                    if wordcloud_fig_report:
                        analyses_for_report['wordcloud_image'] = fig_to_base64(wordcloud_fig_report)
                    else:
                        analyses_for_report['wordcloud_image'] = '<p>No word cloud generated.</p>'
                else:
                    analyses_for_report['wordcloud_image'] = '<p>Word cloud generation was not selected.</p>'

                # 7. Co-occurrence Graph
                if analysis_selections['co_occurrence_network']:
                    co_occurrence_fig_report, co_occurrence_message_report = generate_co_occurrence_graph(combined_feedback_series, min_cooccurrence)
                    if co_occurrence_fig_report:
                        analyses_for_report['co_occurrence_graph'] = fig_to_base64(co_occurrence_fig_report)
                    else:
                        analyses_for_report['co_occurrence_graph'] = f'<p>{co_occurrence_message_report if co_occurrence_message_report else "No co-occurrence graph generated."}</p>'
                else:
                    analyses_for_report['co_occurrence_graph'] = '<p>Co-occurrence network generation was not selected.</p>'

                # 8. Gemini AI Summaries for Report
                overall_summary_report_html = ""
                question_summaries_report_html = ""
                if analysis_selections['gemini_summarization'] and st.session_state.get('gemini_model'):
                    if 'overall_gemini_summary_generated' in st.session_state and st.session_state['overall_gemini_summary_generated']:
                         overall_summary_report_html = f"<h4>Overall Feedback Summary</h4><p>{st.session_state['overall_gemini_summary_generated']}</p>"

                    for col in selected_text_columns:
                        if f'gemini_summary_{col}' in st.session_state and st.session_state[f'gemini_summary_{col}']:
                            question_summaries_report_html += f"<h4>Summary for '{col}'</h4><p>{st.session_state[f'gemini_summary_{col}']}</p>"
                
                analyses_for_report['gemini_summary_overall'] = overall_summary_report_html if overall_summary_report_html else '<p>No overall Gemini summary generated for report.</p>'
                analyses_for_report['gemini_summary_questions'] = question_summaries_report_html if question_summaries_report_html else '<p>No per-question Gemini summaries generated for report.</p>'
                if not analysis_selections['gemini_summarization']:
                    analyses_for_report['gemini_summary_overall'] = '<p>Gemini AI summarization was not selected.</p>'
                    analyses_for_report['gemini_summary_questions'] = ''


                html_report_content = create_html_report(df, selected_text_columns, analyses_for_report, analysis_selections)
                
                b64_html = base64.b64encode(html_report_content.encode()).decode()
                download_filename = f"feedback_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                
                st.sidebar.markdown(
                    f'<a href="data:text/html;base64,{b64_html}" download="{download_filename}">Click to Download HTML Report</a>',
                    unsafe_allow_html=True
                )
                st.sidebar.success("Report generated!")
            
else:
    st.info("Please upload a CSV or Excel file to begin the analysis.")
